{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVDe4oXgHr4O"
   },
   "source": [
    "# Spam Detection folosind Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1529177576962,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "mzQfNUSXWwjR",
    "outputId": "eb325982-fd09-4081-d2f8-e72d6098bfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  spam.csv  spam.csv.1  spam.csv.2  spam.csv.3\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsdLmk70TPjY"
   },
   "source": [
    "Implementam modulul utils ce conține plot_learning_curve, de care ne vom folosi pe parcurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1529177577757,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "cGe1o5VEYLzK",
    "outputId": "d487b00c-79ee-40d0-8f28-ce5eee56d017"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# compute stuff\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Useful to plot the learning curve: training vs cross-validation\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1529177578646,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "BPSAhy9gHr4T",
    "outputId": "a6b48968-1cb5-45ff-d776-a2f6ae1843ab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTnMwP0FHr4c"
   },
   "source": [
    "## AUC ROC\n",
    "\n",
    "Acesta este de fapt un grafic ce va fi plotat cu scopul de a evalua în ce măsură avem specificitate în modelul nostru, versus sensibilitate.\n",
    "\n",
    "**Specificitatea** modelului este dată de rata de true positives (adică dacă eu vreau ca modelul să prezică spam, specificitatea va fi dată de nr de predicții 'spam' care sunt și corecte).\n",
    "\n",
    "**Sensibilitatea** reprezintă cât de mult s-a înșelat modelul din punctul de vedere al sample-urilor pozitive. Adică câte false negatives a dat în faza de test.\n",
    "\n",
    "AUC - Area Under the Curve\n",
    "ROC - Receiver Operating Characteristic\n",
    "\n",
    "Note:\n",
    "- îmi doresc un AUC cu cât mai mult peste 0.5, ca să pot avea încredere în puterea de predicție a modelului meu.\n",
    "- AUC ROC îmi dă niște indicii despre acuratețe, da ele nu reprezintă același lucru. Cu alte cuvinte, pentru seturi de date nebalansate (în care am muuulte sample-uri dintr-o clasă și extrem de puține din alta) nu mai pot avea prea mare încredere în acuratețea overall.\n",
    "\n",
    "Nu trebuie să vă faceți griji dacă nu înțelegeți foarte bine codul acesta de plotare, căci este un helper care ne va ajuta să vizualizăm niște rezultate. Nu influențează cu nimic partea de ML implementată aici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LjdQJMVYHr4d"
   },
   "outputs": [],
   "source": [
    "# Plot roc curve for a specific class\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, rc_msg=\"Test Data\\n\"):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(rc_msg + 'Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7_s22F89Hr4j"
   },
   "outputs": [],
   "source": [
    "def compute_roc_auc(y_test, y_score):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    # We are interested in the false positives rate and also the true positive rate\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2i8MYsRHr4q"
   },
   "source": [
    "## Mean & Standard Deviation\n",
    "\n",
    "Avem aici o funcție care nu face prea multe :)\n",
    "Primește o listă și un mesaj și printează mesajul însoțit de media aritmetică și deviația standard a valorilor din listă.\n",
    "\n",
    "Media aritmetică o să presupun că știți cu ce se mănâncă.\n",
    "Haidem să auzim 3 vorbe despre deviația standard:\n",
    "- ne dă o idee despre precizia modelului nostru.\n",
    "- dacă de exemplu noi avem o listă în care fiecare element reprezintă acuratețea obținută în fiecare rundă de test pe date diferite, ne-ar interesa să știm cam la ce distanță au fost acele acurateți. Asta ne zice deviația standard.\n",
    "- o deviație standard mare, se traduce drept o precizie mai mică.\n",
    "- vrem ca std dev să fie cât mai mic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g-tY3pfcHr4r"
   },
   "outputs": [],
   "source": [
    "def print_mean(vals_lst, msg):\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    print(msg,  np.mean(vals_lst), np.std(vals_lst) )\n",
    "    print(\"-----------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra9HOIGXHr4x"
   },
   "source": [
    "## Un pas din Logistic Regression\n",
    "\n",
    "Avem următoarele etape:\n",
    "\n",
    "1. **Efectuăm o normalizare a feature-urilor pe care le avem.**\n",
    "    Problema pe care o studiem noi aici trimite mult în zona de Natural Language Processing. Aici atributele (features) sunt cuvintele. Problema este că aceste metode pe care le folosim (SVM, Logistic Regression, etc) nu prea se descurcă cu altceva decât numere, așa că este nevoie de treaba asta cu transformarea. Ceea ce este special la tfidf este că el previne acordarea unui scor prea mare (numărul la care se face conversia cuvintelor) acelor cuvinte care apar frecvent dar care au conținut informațional slab.\n",
    "    - 1.1. Există clasa **TfidfVectorizer** care se poate ocupa de tot ce am vorbit noi mai sus.\n",
    "    - 1.2. Vom folosi **fit_transform** ca să ne convertim feature-urile noastre care sunt de fapt cuvinte. Această funcție face de fapt un dicționar în care cheia este cuvântul iar valoarea este feature-ul numeric.\n",
    "    - 1.3. Folosim **transform** pentru datele de test **X_te**, pentru că aici nu ne mai trebuie acel dicționar, ci doar feature-urile transformate numeric.\n",
    "\n",
    "2. \n",
    "    - 2.1. **Alegem ce parametri vrem să optimizăm.** \n",
    "   Noi vom opta pentru **C-value** aici, acesta fiind foarte similar ca și semnificație și interpretare cu C-value-ul de la SVM despre care am discutat adineauri în cadrul acestui workshop.\n",
    "   \n",
    "    - 2.2. **Inițializăm modelul**.\n",
    "    Aici vom folosi **LogisticRegression** din sklearn, pe care l-am importat deja mai sus.\n",
    "    Ca și parametru, vom avea **random_state**=100 (seed-ul folosit de pseudo random number generator-ul folosit când se face shuffle la date).\n",
    "3. **Inițializăm un obiect de tipul GridSearchCV** \n",
    "    acesta va fi în stare să folosească un clasificator custom (în cazul nostru **clf**) și să optimizeze o metrică aleasă (în cazul nostru **accuracy**), iar ca să facă asta o să ne aleagă cel mai bun parametru dintr-o mulțime dată (în cazul nostru acel C-score din mulțimea **p_grid**). \n",
    "4. După ce am făcut fit la date în grid, o să avem deja un best_score_ și best_params_:\n",
    "    - 4.1. adăugăm best_score_ la finalul listei cu acurateti de validare, pe care o mărim cu 1 elem la fiecare pas din SVM.\n",
    "    - 4.2. Potrivim cei mai buni parametri grid.best_params_ în clf (practic re-inițializăm clf la un LogisticRegression mai tunat).\n",
    "5. Începem să punem clasificatorul optimizat la treabă:\n",
    "    - 5.1. potrivim (**fit**) **X_tr** și **Y_tr** în clasificator (**clf**).\n",
    "    - 5.2. stocăm în **preds_fold** predicțiile lui clf pe datele de test **X_te**. Obținem aceste **predicții binare** (aparține/nu aparține clasei spam/ham) folosind funcția **predict** implementată în LogisticRegression. \n",
    "    - 5.3. folosim **predict_proba** din **clf** și cu **X_te** drept parametru ca să obținem încrederea în fiecare predicție (cât la sută crede modelul meu că un sample dat poate fi clasificat drept spam/ham).\n",
    "6. Se stochează predicțiile (atât cele binare cât și cele sub-unitare aka procente normalizate la 1).\n",
    "   - **NOTĂ**: se folosește aici funcția **extend** pentru că ceea ce obținem noi din predict/decision_function sunt liste. Iar noi vrem să extindem o listă curentă cu elementele din listele respective ci nu să adăugăm (append) listele acelea la lista finală de predicții. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8Ws9fL8QHr4z"
   },
   "outputs": [],
   "source": [
    "def lr_useful_work(X_tr, Y_tr, X_te, Y_te, preds, preds_scores, f1_folds, val_accuracy):\n",
    "    # 1.1. Vectorise for the current fold\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r'(?u)\\b[@#]?\\w[\\w_]+\\b',)\n",
    "\n",
    "    # TODO 1.2. The new X_tr becomes a dictionary {word: numeric_feature}\n",
    "    #      in order to obtain this, use fit_transform applied on vectorizer and with the old X_tr as parameter\n",
    "\n",
    "    \n",
    "    # TODO 1.3. Simply re-initialize X_te to a feature vector obtained using the transform function from vectorizer \n",
    "\n",
    "\n",
    "    # 2.1. Params to optimise\n",
    "    p_grid = {\"C\": [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5]}\n",
    "\n",
    "    # TODO 2.2.. The model that will be transformed into a model able to differentiate between two classes\n",
    "    # use LogisticRegression as classifier, with random_state=100 as parameter\n",
    "\n",
    "\n",
    "    # TODO 3. initialize the grid\n",
    "    # use GridSearchCV, with: clf as estimator, p_grid as param_grid and accuracy as scoring measure \n",
    "\n",
    "    \n",
    "    # TODO 4. The grid declared earlier has a method 'fit' \n",
    "    # fit is useful in **fitting** the training data to the requirements of the model\n",
    "    # use it to fit X_tr and Y_tr to the model\n",
    "\n",
    "    \n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    \n",
    "    # TODO 4.1. Append the best_score_ in grid to the validation accuracy list val_accuracy\n",
    "\n",
    "\n",
    "    # TODO 4.2. Re-initialize clf, but this time you know that for the C parameter, the optimal grid.best_params_['C']\n",
    "    #      discovered above should be used.\n",
    "\n",
    "\n",
    "    # TODO 5.1. Fit the training data X_tr and Y_tr to the classifier \n",
    "\n",
    "\n",
    "    # TODO 5.2. Use predict (a method that can be applied on clf) with the test data X_te \n",
    "    #      to get the predictions (0, 1) for each class\n",
    "\n",
    "    \n",
    "    # 5.3. use predict_proba (also from clf and with X_te as parameter) to get scores for each class\n",
    "    #      So unlike predict, predict_proba will give the confidence in percents (normalized to 1) in each class. \n",
    "    preds_fold_scores = clf.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    f1_folds.append(f1_score(Y_te, preds_fold))\n",
    "\n",
    "    # TODO 6.1. store the predictions (preds) obtained in this step for future computations \n",
    "    #      regarding the performance of the model aka extend preds with preds_fold\n",
    "\n",
    "    # TODO 6.2. extend preds_scores with preds_fold_scores\n",
    "\n",
    "    \n",
    "    return vectorizer, clf, preds, preds_scores, f1_folds, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MfOKO9kFHr46"
   },
   "outputs": [],
   "source": [
    "def run_lr(X, Y, skf):\n",
    "    fold_count = 1\n",
    "    f1_folds = []\n",
    "    val_accuracy = []\n",
    "    preds = []\n",
    "    preds_scores = []\n",
    "    # ground truth\n",
    "    test_gt = []\n",
    "\n",
    "    X_tr_all = []\n",
    "    Y_tr_all = []\n",
    "    \n",
    "    # nested cross-validation\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        print(\"Fold \", fold_count)\n",
    "        print(\"----\")\n",
    "        fold_count+=1\n",
    "\n",
    "        # split into train and test\n",
    "        X_tr, Y_tr = X[train_index], Y[train_index]\n",
    "        X_te, Y_te = X[test_index], Y[test_index]\n",
    "        \n",
    "        X_tr_all.extend(X_tr)\n",
    "        Y_tr_all.extend(Y_tr)\n",
    "\n",
    "        vectorizer, clf, preds, preds_scores, f1_folds, val_accuracy = \\\n",
    "        lr_useful_work(X_tr, Y_tr, X_te, Y_te, preds, preds_scores, f1_folds, val_accuracy)\n",
    "        \n",
    "        test_gt += Y_te.tolist()\n",
    "        \n",
    "        print (\"----\")\n",
    "        \n",
    "    return X_tr_all, Y_tr_all, preds, preds_scores, test_gt, f1_folds, val_accuracy, vectorizer, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDAQGASIHr4_"
   },
   "source": [
    "## Metrici și Grafice de Interes\n",
    "\n",
    "### 1. Acuratețe\n",
    "\n",
    "Număr de predicții corecte din nr total de predicții.\n",
    "\n",
    "### 2. F1-Score\n",
    "\n",
    "Media armonică dintre precizie și recall:\n",
    "2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "Acesta se uită atât la exemplele pozitive cât și la cele negative din dataset. Deci ne dă o încredere mai mare în performanță dacă avem de-a face cu un dataset neechilibrat (asemeni AUC ROC).\n",
    "\n",
    "### 3. Curbă de Învățare\n",
    "\n",
    "Curba de învățare ne spune cât de bine s-a descurcat modelul în faza de training. Sau, altfel spus, cât de bine a învățat acesta.\n",
    "\n",
    "### 4. Raport de Clasificare\n",
    "\n",
    "Ne arată următoarele valori, pentru fiecare clasă (în cazul nostru SPAM/HAM):\n",
    "\n",
    "- **precizie** = tp / (tp + fp). Adică numărul de true positives (exemple clasificate corect ca aparținând clasei 1) din câte positive a clasificat de fapt modelul. Ne zice de fapt puterea de predicție a sample-urilor pozitive.\n",
    "- **recall**: = tp / (tp + fn). Adică câte am clasificat drept pozitive din câte pozitive erau de fapt.\n",
    "- **f1-score**: vezi mai sus.\n",
    "- **suport**: câte sample-uri avem pe care s-a calculat respectivul scor.\n",
    "\n",
    "### 5. AUC ROC\n",
    "\n",
    "Vrem să fie cu cât mai mult peste 0.5, ca să putem avea încredere în puterea de predicție a modelului.\n",
    "\n",
    "AUC ROC dă indicii despre acuratețe, dar nu avem termen de comparație cu acuratețea . Cu alte cuvinte, pentru seturi de date nebalansate (în care am muuulte sample-uri dintr-o clasă și extrem de puține din alta) nu mai pot avea prea mare încredere în acuratețea overall. Atunci AUC ROC îmi vine-n ajutor.\n",
    "\n",
    "\n",
    "### 6. Matrice de Confuzie\n",
    "\n",
    "În general o să ne dorim o culoare cât mai închisă pe diagonala principală. Aici vom avea True Positives și True Negatives. Diagonala secundară, în cazul de clasificare binară va conține numărul de False Positives și False Negatives.\n",
    "Analizați graficele odată ce le obțineți, pentru o mai bună (si mai vizuală) înțelegere a situației."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z8PQuDsBHr5B"
   },
   "outputs": [],
   "source": [
    "def print_results(X_tr_all, Y_tr_all, preds, preds_scores, test_gt, f1_folds, val_accuracy, vectorizer, clf,\\\n",
    "                 lc_msg=\"Train on all categories\\n\", cm_msg=\"Test on all categories\\n\"):\n",
    "    \n",
    "    # TODO 1. Print the 'Mean Validation Accuracy' using print_mean implemented above on the val_accuracy list\n",
    "\n",
    "    \n",
    "    # TODO 2. Print the 'Mean F1-score' using print_mean implemented above on the f1_folds list\n",
    "    \n",
    "\n",
    "    # 3. Plot the Learning Curve\n",
    "    X_tr_all = vectorizer.fit_transform(X_tr_all)\n",
    "    plot_learning_curve(clf, lc_msg + \"Accuracy vs. Training Set Size\", X_tr_all, np.array(Y_tr_all), cv=10)\n",
    "    \n",
    "    # TODO 4. Print the overall performance of the model using classification_report from sklearn\n",
    "    #    as parameters here you'll have: \n",
    "    #    - the ground truth for the test set (test_gt)\n",
    "    #    - the classes predicted by the model (preds)\n",
    "    #    - target_names=['HAM', 'SPAM'] - to have the labels nicely printed in the report\n",
    "    print (\"-----------------------------------------------------------------------------\")\n",
    "    print (\"Classification Report\")\n",
    "    print (\"-----------------------------------------------------------------------------\")\n",
    "    print(\"todo\")\n",
    "    print (\"-----------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # 5.1. Let the compute_roc_auc boss implemented above compute the false positives rate, true pos rate and roc_auc.\n",
    "    #      Do not forget to feed it the test_gt and preds_scores as parameters!\n",
    "    fpr, tpr, roc_auc = compute_roc_auc(test_gt, preds_scores)\n",
    "    # TODO 5.2. Plot the AUC ROC using plot_roc_curve defined above\n",
    "\n",
    "    \n",
    "    # 6. Plot the Confusion Matrix\n",
    "    cm = confusion_matrix(test_gt, preds)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm)\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09So6RLHr5H"
   },
   "source": [
    "### Model inspection\n",
    "\n",
    "Ne zice care sunt cele mai predictive features găsite de model. În cazul nostru, care sunt cuvintele pe care modelul a ajuns să le coreleze cel mai tare cu spam/ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dKjL-8HsHr5J"
   },
   "outputs": [],
   "source": [
    "def model_inspection(clf, vectorizer):\n",
    "    #NOTE THAT THIS MODEL IS THE ONE FROM THE LAST FOLD.\n",
    "    # If we want to be more accurate, we have to train use all of the data and then inspect\n",
    "    bow_feat_names = np.array(vectorizer.get_feature_names())\n",
    "    print(\"Number of non zero coefficients:\", np.count_nonzero(clf.coef_[0,:]))\n",
    "    print(\"\\nTop-20 most predicive feats for each class\")\n",
    "    print('------')\n",
    "\n",
    "    feats = np.argsort(clf.coef_[0,:])[::-1]\n",
    "    print(\"Class SPAM:\", bow_feat_names[feats[:20]])\n",
    "    print()\n",
    "    feats = np.argsort(clf.coef_[0,:])\n",
    "    print(\"Class HAM:\", bow_feat_names[feats[:20]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2068,
     "status": "ok",
     "timestamp": 1529177588674,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "WTbaV5c3Hr5Q",
    "outputId": "72455d2d-7701-4cb6-94a1-ca8b9ee51c44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "!wget 'http://codette.ro/files/spam.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t_rOBa2hR-k_"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'spam.csv' does not exist: b'spam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-40df7ba069c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'spam.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'spam.csv' does not exist: b'spam.csv'"
     ]
    }
   ],
   "source": [
    "data_file = 'spam.csv'\n",
    "\n",
    "data = pd.read_csv(data_file, quotechar='\"', encoding = \"ISO-8859-1\", header=None).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pl7whe3hHr5b"
   },
   "outputs": [],
   "source": [
    "# The labels (spam/ham) are loaded and converted to binary labels (1/0)\n",
    "# X_test = data[:573,1].astype(str)\n",
    "# X = data[573:,1].astype(str)\n",
    "X = data[:,1].astype(str)\n",
    "# Y_temp_test = data[:573,0].astype(np.str)\n",
    "# Y_temp = data[573:,0].astype(np.str)\n",
    "Y_temp = data[:,0].astype(np.str)\n",
    "\n",
    "# Y_test = np.zeros(len(Y_temp_test))\n",
    "Y = np.zeros(len(Y_temp))\n",
    "for i in range(len(Y_temp)):\n",
    "    if Y_temp[i] == 'spam':\n",
    "        Y[i] = 1\n",
    "        \n",
    "# for i in range(len(Y_temp_test)):\n",
    "#     if Y_temp_test[i] == 'spam':\n",
    "#         Y_test[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15847,
     "status": "ok",
     "timestamp": 1529177606823,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "wVlQtMl_Hr5n",
    "outputId": "6d00417b-7a90-40a9-a5be-8ce2f7e6a4df"
   },
   "outputs": [],
   "source": [
    "# Build and Train the classifier\n",
    "skf = StratifiedKFold(n_splits=10, random_state=100)\n",
    "X_tr_all, Y_tr_all, preds, preds_scores, test_gt, f1_folds, val_accuracy, vectorizer, clf = run_lr(X, Y, skf)\n",
    "\n",
    "# preds_test = clf.predict(vectorizer.transform(X_test))\n",
    "# preds_test_scores = clf.predict_proba(vectorizer.transform(X_test))[:, 1]\n",
    "# f1_test = f1_score(Y_test, preds_test)\n",
    "# val_acc_test = accuracy_score(Y_test, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1458
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3761,
     "status": "ok",
     "timestamp": 1529177653364,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "30a0-kKGHr53",
    "outputId": "f7d14cac-2ad3-4c3a-9e0b-f3a00df9b2bb"
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "#print_results(X_test, Y_test, preds_test, preds_test_scores, Y_test, f1_test, val_acc_test, vectorizer, clf)\n",
    "# Print the results\n",
    "print_results(X, Y, preds, preds_scores, test_gt, f1_folds, val_accuracy, vectorizer, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1529177660579,
     "user": {
      "displayName": "Mihaela Găman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105187480298227677604"
     },
     "user_tz": -180
    },
    "id": "2Lfm7OtdHr6B",
    "outputId": "d42c0188-8c85-4351-9220-8371151da8ee"
   },
   "outputs": [],
   "source": [
    "# Model inspection\n",
    "model_inspection(clf, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "logistic_regression.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
